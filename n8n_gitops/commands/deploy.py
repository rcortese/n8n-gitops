"""Deploy command implementation."""

import argparse
import json
from pathlib import Path
from typing import Any

from datetime import datetime

from n8n_gitops import logger
from n8n_gitops.config import load_auth
from n8n_gitops.exceptions import ManifestError, RenderError
from n8n_gitops.gitref import create_snapshot
from n8n_gitops.manifest import load_manifest
from n8n_gitops.n8n_client import N8nClient
from n8n_gitops.render import RenderOptions, render_workflow_json


def _sync_tags(
    client: N8nClient,
    manifest_tags: dict[str, str],
) -> tuple[dict[str, str], bool]:
    """Synchronize tags between manifest and n8n instance.

    Args:
        client: N8n API client
        manifest_tags: Tag ID to name mapping from manifest

    Returns:
        Tuple of (updated_tags_mapping, tags_were_created)
        - updated_tags_mapping: Updated tag ID to name mapping (with new IDs for created tags)
        - tags_were_created: True if any tags were created (manifest needs updating)
    """
    logger.info("\nSynchronizing tags...")

    # Fetch existing tags from n8n
    try:
        remote_tags = client.list_tags()
        logger.info(f"Found {len(remote_tags)} remote tag(s)")
    except Exception as e:
        logger.warning(f"  âš  Warning: Could not fetch tags from n8n: {e}")
        return manifest_tags, False

    # Build remote tag mappings
    remote_tags_by_id: dict[str, str] = {}
    remote_tags_by_name: dict[str, str] = {}

    for tag in remote_tags:
        tag_id = tag.get("id")
        tag_name = tag.get("name")
        if tag_id and tag_name:
            remote_tags_by_id[str(tag_id)] = str(tag_name)
            remote_tags_by_name[str(tag_name)] = str(tag_id)

    # Track changes
    updated_tags = dict(manifest_tags)  # Copy manifest tags
    tags_were_created = False

    # Process each tag from manifest
    for tag_id, tag_name in manifest_tags.items():
        if tag_id in remote_tags_by_id:
            # Tag ID exists in n8n
            remote_name = remote_tags_by_id[tag_id]
            if remote_name != tag_name:
                # Name is different, update it
                logger.info(f"  ðŸ”„ Updating tag '{tag_id}': '{remote_name}' â†’ '{tag_name}'")
                try:
                    client.update_tag(tag_id, tag_name)
                    logger.info("    âœ“ Updated")
                except Exception as e:
                    logger.error(f"    âœ— Failed to update tag: {e}")
            else:
                # Name matches, nothing to do
                logger.info(f"  âœ“ Tag '{tag_name}' (ID: {tag_id}) already up to date")
        else:
            # Tag ID doesn't exist in n8n, create new tag
            logger.info(f"  âž• Creating tag '{tag_name}' (manifest ID '{tag_id}' not found in n8n)")
            try:
                created_tag = client.create_tag(tag_name)
                new_tag_id = created_tag.get("id")

                if new_tag_id:
                    logger.info(f"    âœ“ Created with new ID: {new_tag_id}")
                    # Update the mapping with the new ID
                    updated_tags.pop(tag_id)  # Remove old ID
                    updated_tags[str(new_tag_id)] = tag_name
                    tags_were_created = True
                else:
                    logger.error("    âœ— Created tag but no ID returned")
            except Exception as e:
                logger.error(f"    âœ— Failed to create tag: {e}")

    return updated_tags, tags_were_created


def _prepare_workflow_for_api(workflow: dict[str, Any]) -> dict[str, Any]:
    """Prepare workflow for n8n API by removing fields that cause validation errors.

    Args:
        workflow: Workflow object

    Returns:
        Cleaned workflow ready for API submission
    """
    import copy
    cleaned = copy.deepcopy(workflow)

    # Remove fields that n8n API doesn't accept or are auto-generated
    # These are readonly fields managed by n8n
    fields_to_remove = [
        "id",           # Auto-generated by n8n
        "createdAt",    # Auto-generated timestamp
        "updatedAt",    # Auto-generated timestamp
        "versionId",    # Version control field
        "shared",       # Sharing/permissions data
        "isArchived",   # Archive status (managed separately)
        "active",       # Active state (set via separate PATCH request)
        "tags",         # Tags (read-only in PUT, managed separately)
        "meta",         # Metadata (if null, causes issues)
        "pinData",      # Pinned test data (if empty, causes issues)
        "staticData",   # Static data (if null, causes issues)
        "triggerCount",  # Trigger counter
    ]

    for field in fields_to_remove:
        cleaned.pop(field, None)

    # Also remove null/empty fields that can cause issues
    # pinData, meta, staticData if they're null or empty
    if cleaned.get("meta") is None:
        cleaned.pop("meta", None)
    if cleaned.get("pinData") == {}:
        cleaned.pop("pinData", None)
    if cleaned.get("staticData") is None:
        cleaned.pop("staticData", None)

    return cleaned


def run_deploy(args: argparse.Namespace) -> None:
    """Deploy workflows to n8n instance.

    Args:
        args: CLI arguments

    Raises:
        SystemExit: If deployment fails
    """
    repo_root = Path(args.repo_root).resolve()
    n8n_root = "n8n"

    # Load auth config
    try:
        auth = load_auth(repo_root, args)
    except Exception as e:
        logger.critical(f"Error: {e}")

    # Create snapshot
    snapshot = create_snapshot(repo_root, args.git_ref)

    logger.info(f"Deploying workflows from {repo_root}")
    if args.git_ref:
        logger.info(f"Using git ref: {args.git_ref}")
    logger.info(f"Target: {auth.api_url}")
    logger.info("")

    # Load manifest
    try:
        manifest = load_manifest(snapshot, n8n_root)
        logger.info(f"Loaded manifest: {len(manifest.workflows)} workflow(s)")
    except ManifestError as e:
        logger.critical(f"Error loading manifest: {e}")

    # Initialize client
    client = N8nClient(auth.api_url, auth.api_key)

    # Synchronize tags (update names, create missing tags)
    updated_tags, tags_were_created = _sync_tags(client, manifest.tags)

    # Update manifest tags and workflow tag references if any were created (new IDs assigned)
    if tags_were_created:
        # Build old_id -> new_id mapping
        old_ids = set(manifest.tags.keys())
        new_ids = set(updated_tags.keys())

        # Find which IDs changed
        id_mapping: dict[str, str] = {}
        for old_id in old_ids:
            if old_id not in new_ids:
                # This old ID was replaced, find the new one
                old_name = manifest.tags[old_id]
                for new_id, new_name in updated_tags.items():
                    if new_name == old_name and new_id not in old_ids:
                        id_mapping[old_id] = new_id
                        break

        # Update tag IDs in workflow specs
        if id_mapping:
            logger.info(f"\n  Updating tag references in {len(manifest.workflows)} workflow(s)...")
            for spec in manifest.workflows:
                updated_tag_ids = []
                for tag_id in spec.tags:
                    # Use new ID if it was mapped, otherwise keep old ID
                    new_tag_id = id_mapping.get(tag_id, tag_id)
                    updated_tag_ids.append(new_tag_id)
                spec.tags = updated_tag_ids

        # Update manifest tags mapping
        manifest.tags = updated_tags

        logger.warning("\nâš  Tags were created with new IDs - manifest needs updating")
        logger.warning("  Run 'n8n-gitops export' to update the manifest with new tag IDs")

    # Fetch remote workflows
    logger.info("\nFetching remote workflows...")
    try:
        remote_workflows = client.list_workflows()
        logger.info(f"Found {len(remote_workflows)} remote workflow(s)")
    except Exception as e:
        logger.critical(f"Error fetching remote workflows: {e}")

    # Create name -> id mapping
    name_to_id: dict[str, str] = {}
    for wf in remote_workflows:
        name = wf.get("name")
        wf_id = wf.get("id")
        if name and wf_id:
            name_to_id[name] = wf_id

    # Plan deployment
    plan: list[dict[str, Any]] = []

    for spec in manifest.workflows:
        workflow_path = f"{n8n_root}/{spec.file}"

        # Load workflow
        try:
            workflow_json = snapshot.read_text(workflow_path)
            workflow = json.loads(workflow_json)
        except Exception as e:
            logger.critical(f"Error loading workflow {spec.name}: {e}")

        # Render with includes
        render_options = RenderOptions(
            enforce_no_inline_code=False,
            enforce_checksum=False,
            require_checksum=False,
            add_generated_header=False,
        )

        try:
            rendered, reports = render_workflow_json(
                workflow,
                snapshot,
                n8n_root=n8n_root,
                git_ref=args.git_ref,
                options=render_options,
            )
        except RenderError as e:
            logger.critical(f"Error rendering workflow {spec.name}: {e}")

        # Ensure name matches manifest
        rendered["name"] = spec.name

        # Determine action (default is replace: delete old + create new)
        if spec.name in name_to_id:
            action = "replace"  # Delete old workflow and create new one
            workflow_id = name_to_id[spec.name]
        else:
            action = "create"
            workflow_id = None

        plan.append(
            {
                "spec": spec,
                "workflow": rendered,
                "action": action,
                "workflow_id": workflow_id,
                "reports": reports,
            }
        )

    # Handle --prune: find workflows to delete
    workflows_to_prune = []
    if args.prune:
        manifest_names = {spec.name for spec in manifest.workflows}
        for wf in remote_workflows:
            wf_name = wf.get("name")
            if wf_name and wf_name not in manifest_names:
                workflows_to_prune.append(wf)

    # Print plan
    logger.info("\nDeployment plan:")
    for item in plan:
        spec = item["spec"]
        action = item["action"]
        if action == "create":
            logger.info(f"  + CREATE: {spec.name}")
        elif action == "replace":
            if args.backup:
                logger.info(f"  âŸ³ REPLACE (with backup): {spec.name}")
            else:
                logger.info(f"  âŸ³ REPLACE: {spec.name}")

        for report in item["reports"]:
            if report.status == "included":
                logger.info(f"      âœ“ Include: {report.include_path}")

    if workflows_to_prune:
        logger.info(f"\n  ðŸ—‘  PRUNE: {len(workflows_to_prune)} workflow(s) not in manifest:")
        for wf in workflows_to_prune:
            logger.info(f"      - {wf.get('name')}")

    # Dry run check
    if args.dry_run:
        logger.info("\n[DRY RUN] No changes made")
        raise SystemExit(0)

    # Execute deployment
    logger.info("\nExecuting deployment...")
    for item in plan:
        spec = item["spec"]
        workflow = item["workflow"]
        action = item["action"]
        workflow_id = item["workflow_id"]

        try:
            # Prepare workflow for API (remove fields that cause validation errors)
            api_workflow = _prepare_workflow_for_api(workflow)

            if action == "create":
                logger.info(f"  Creating: {spec.name}...")
                result = client.create_workflow(api_workflow)
                workflow_id = result.get("id")
                logger.info(f"    âœ“ Created with ID: {workflow_id}")

            elif action == "replace":
                # Replace mode: delete old workflow and create new one
                logger.info(f"  Replacing: {spec.name}...")

                # Backup old workflow if requested
                if args.backup:
                    timestamp = datetime.now().strftime("%Y-%m-%d %H:%M:%S")
                    backup_name = f"[BKP {timestamp}] {spec.name}"
                    logger.info(f"    Backing up old workflow as: {backup_name}")

                    # Rename old workflow
                    old_workflow = client.get_workflow(workflow_id)
                    old_workflow["name"] = backup_name
                    old_workflow_cleaned = _prepare_workflow_for_api(old_workflow)
                    client.update_workflow(workflow_id, old_workflow_cleaned)
                    logger.info("    âœ“ Backup created")

                    # Now create new workflow with original name
                    logger.info("    Creating new workflow...")
                    result = client.create_workflow(api_workflow)
                    workflow_id = result.get("id")
                    logger.info(f"    âœ“ Created with ID: {workflow_id}")
                else:
                    # Delete old workflow and create new one
                    logger.info("    Deleting old workflow...")
                    try:
                        client.delete_workflow(workflow_id)
                        logger.info("    âœ“ Old workflow deleted")
                    except Exception as e:
                        logger.warning(f"    âš  Could not delete old workflow: {e}")
                        logger.warning("    â†’ Creating new workflow anyway...")

                    logger.info("    Creating new workflow...")
                    result = client.create_workflow(api_workflow)
                    workflow_id = result.get("id")
                    logger.info(f"    âœ“ Created with ID: {workflow_id}")

            # Set active state based on manifest
            if workflow_id:
                if spec.active:
                    logger.info("    Activating workflow...")
                    client.activate_workflow(workflow_id)
                    logger.info("    âœ“ Activated")
                else:
                    logger.info("    Deactivating workflow...")
                    client.deactivate_workflow(workflow_id)
                    logger.info("    âœ“ Deactivated")

                # Update workflow tags
                if spec.tags:
                    logger.info(f"    Updating tags ({len(spec.tags)} tag(s))...")
                    client.update_workflow_tags(workflow_id, spec.tags)
                    logger.info("    âœ“ Tags updated")

        except Exception as e:
            logger.error(f"    âœ— Error: {e}")

            # Provide helpful suggestions for common errors
            error_str = str(e).lower()
            if "additional properties" in error_str or "validation" in error_str:
                logger.error("\n    ðŸ’¡ Tip: The workflow file may contain n8n-managed fields.")
                logger.error("    Run 'n8n-gitops validate' to check for problematic fields.")
                logger.error("    Re-export the workflow to get a clean version:")
                logger.error(f"      n8n-gitops export --names \"{spec.name}\" --externalize-code")

            raise SystemExit(1)

    # Execute prune if requested
    if workflows_to_prune:
        logger.info("\nPruning workflows not in manifest...")
        for wf in workflows_to_prune:
            wf_id = wf.get("id")
            wf_name = wf.get("name")
            try:
                logger.info(f"  Deleting: {wf_name}...")
                client.delete_workflow(wf_id)
                logger.info("    âœ“ Deleted")
            except Exception as e:
                logger.error(f"    âœ— Error deleting {wf_name}: {e}")

    logger.info("\nâœ“ Deployment successful!")
