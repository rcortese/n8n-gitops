"""Export command implementation."""

import argparse
import json
import re
import shutil
from pathlib import Path
from typing import Any

import yaml

from n8n_gitops.config import load_auth
from n8n_gitops.gitref import WorkingTreeSnapshot
from n8n_gitops.manifest import load_manifest
from n8n_gitops.n8n_client import N8nClient
from n8n_gitops.normalize import normalize_json, strip_volatile_fields
from n8n_gitops.render import CODE_FIELD_NAMES


def run_export(args: argparse.Namespace) -> None:
    """Export workflows from n8n instance.

    Args:
        args: CLI arguments

    Raises:
        SystemExit: If export fails
    """
    repo_root = Path(args.repo_root).resolve()
    n8n_root = repo_root / "n8n"
    workflows_dir = n8n_root / "workflows"
    manifests_dir = n8n_root / "manifests"
    scripts_dir = n8n_root / "scripts"
    manifest_file = manifests_dir / "workflows.yaml"

    # Ensure directories exist
    workflows_dir.mkdir(parents=True, exist_ok=True)
    manifests_dir.mkdir(parents=True, exist_ok=True)
    scripts_dir.mkdir(parents=True, exist_ok=True)

    # Load auth config
    try:
        auth = load_auth(repo_root, args)
    except Exception as e:
        print(f"Error: {e}")
        raise SystemExit(1)

    print(f"Exporting workflows from {auth.api_url}")
    print(f"Target directory: {workflows_dir}")
    print()

    # Initialize client
    client = N8nClient(auth.api_url, auth.api_key)

    # Fetch workflows
    print("Fetching workflows...")
    try:
        remote_workflows = client.list_workflows()
        print(f"Found {len(remote_workflows)} workflow(s)")
    except Exception as e:
        print(f"Error fetching workflows: {e}")
        raise SystemExit(1)

    if not remote_workflows:
        print("No workflows found to export")
        raise SystemExit(0)

    # Determine which workflows to export
    workflows_to_export: list[dict[str, Any]] = []

    if args.all:
        workflows_to_export = remote_workflows
    elif args.names:
        # Parse comma-separated names
        requested_names = [n.strip() for n in args.names.split(",")]
        for wf in remote_workflows:
            if wf.get("name") in requested_names:
                workflows_to_export.append(wf)
        # Check for missing workflows
        found_names = {wf.get("name") for wf in workflows_to_export}
        missing = set(requested_names) - found_names
        if missing:
            print(f"Warning: Workflows not found: {', '.join(missing)}")
    elif args.from_manifest:
        # Load manifest and export only those workflows
        snapshot = WorkingTreeSnapshot(repo_root)
        try:
            manifest = load_manifest(snapshot, "n8n")
            manifest_names = {spec.name for spec in manifest.workflows}
            for wf in remote_workflows:
                if wf.get("name") in manifest_names:
                    workflows_to_export.append(wf)
        except Exception as e:
            print(f"Error loading manifest: {e}")
            raise SystemExit(1)
    else:
        print("Error: Must specify --all, --names, or --from-manifest")
        raise SystemExit(1)

    if not workflows_to_export:
        print("No workflows selected for export")
        raise SystemExit(0)

    print(f"\nExporting {len(workflows_to_export)} workflow(s)...")
    if args.externalize_code:
        print("Code externalization: ENABLED")
    else:
        print("Code externalization: DISABLED (use --externalize-code to enable)")

    # Export each workflow
    exported_specs: list[dict[str, Any]] = []
    total_externalized = 0

    for wf_summary in workflows_to_export:
        wf_id = wf_summary.get("id")
        wf_name = wf_summary.get("name")

        if not wf_id or not wf_name:
            print(f"  âš  Skipping workflow with missing id or name")
            continue

        print(f"  Exporting: {wf_name}")

        # Fetch full workflow
        try:
            workflow = client.get_workflow(wf_id)
        except Exception as e:
            print(f"    âœ— Error fetching workflow: {e}")
            continue

        # Strip volatile and n8n-managed fields to ensure clean exports
        # These fields are auto-generated by n8n and cause API validation errors
        # Note: 'active' is kept in the workflow JSON for reference,
        # but stripped during deployment (it's also in the manifest)
        workflow_cleaned = strip_volatile_fields(
            workflow,
            fields=[
                "id",
                "createdAt",
                "updatedAt",
                "versionId",
                "shared",
                "isArchived",
                "triggerCount",
            ],
        )

        # Externalize code if requested
        if args.externalize_code:
            workflow_cleaned, externalized_count = _externalize_workflow_code(
                workflow_cleaned,
                wf_name,
                scripts_dir,
            )
            total_externalized += externalized_count
            if externalized_count > 0:
                print(f"    âœ“ Externalized {externalized_count} code block(s)")

        # Normalize JSON
        normalized_json = normalize_json(workflow_cleaned)

        # Determine filename (sanitize name)
        safe_name = _sanitize_filename(wf_name)
        filename = f"{safe_name}.json"
        filepath = workflows_dir / filename

        # Write file
        try:
            filepath.write_text(normalized_json)
            print(f"    âœ“ Saved to: n8n/workflows/{filename}")
        except Exception as e:
            print(f"    âœ— Error writing file: {e}")
            continue

        # Add to manifest
        exported_specs.append(
            {
                "name": wf_name,
                "file": f"workflows/{filename}",
                "active": workflow.get("active", False),
                "tags": workflow.get("tags", []),
            }
        )

    # Clean up local workflows/scripts that don't exist remotely (mirror mode)
    if args.all:
        print("\nCleaning up local files not in remote...")
        exported_names = {spec["name"] for spec in exported_specs}

        # Clean up workflow files
        if workflows_dir.exists():
            for workflow_file in workflows_dir.glob("*.json"):
                # Try to determine workflow name from file
                try:
                    workflow_data = json.loads(workflow_file.read_text())
                    local_name = workflow_data.get("name")

                    if local_name and local_name not in exported_names:
                        print(f"  ðŸ—‘  Deleting local workflow not in remote: {local_name}")
                        workflow_file.unlink()

                        # Also delete corresponding script directory
                        safe_name = _sanitize_filename(local_name)
                        script_dir = scripts_dir / safe_name
                        if script_dir.exists() and script_dir.is_dir():
                            shutil.rmtree(script_dir)
                            print(f"      â†’ Deleted scripts directory: scripts/{safe_name}/")
                except Exception as e:
                    print(f"  âš  Warning: Could not process {workflow_file.name}: {e}")

        # Clean up orphaned script directories (scripts without corresponding workflow)
        if scripts_dir.exists():
            for script_dir in scripts_dir.iterdir():
                if script_dir.is_dir():
                    # Check if there's a corresponding workflow
                    dir_name = script_dir.name
                    # Check if any exported workflow would use this directory
                    has_match = any(
                        _sanitize_filename(spec["name"]) == dir_name
                        for spec in exported_specs
                    )
                    if not has_match:
                        print(f"  ðŸ—‘  Deleting orphaned scripts directory: scripts/{dir_name}/")
                        shutil.rmtree(script_dir)

    # Update manifest if --all mode
    if args.all and exported_specs:
        print("\nUpdating manifest...")

        # In --all mode, replace manifest completely with exported workflows (mirror mode)
        # This ensures deleted workflows are removed from manifest
        existing_specs = exported_specs

        # Write manifest
        manifest_content = yaml.dump(
            {"workflows": existing_specs},
            default_flow_style=False,
            sort_keys=False,
        )
        try:
            manifest_file.write_text(manifest_content)
            print(f"  âœ“ Updated manifest: {manifest_file.relative_to(repo_root)}")
        except Exception as e:
            print(f"  âœ— Error writing manifest: {e}")

    print(f"\nâœ“ Export complete! Exported {len(exported_specs)} workflow(s)")
    if total_externalized > 0:
        print(f"âœ“ Externalized {total_externalized} code block(s) to script files")
    print("\nNext steps:")
    print("  1. Review the exported workflows")
    if total_externalized > 0:
        print("  2. Review the externalized scripts in n8n/scripts/")
        print("  3. git add n8n/")
        print("  4. git commit -m 'Export workflows from n8n with externalized code'")
    else:
        print("  2. git add n8n/")
        print("  3. git commit -m 'Export workflows from n8n'")


def _sanitize_filename(name: str) -> str:
    """Sanitize workflow name for use as filename.

    Args:
        name: Workflow name

    Returns:
        Sanitized filename (without extension)
    """
    # Replace spaces and special characters with underscores
    safe = re.sub(r"[^\w\-.]", "_", name)
    # Remove multiple underscores
    safe = re.sub(r"_+", "_", safe)
    # Remove leading/trailing underscores
    safe = safe.strip("_")
    return safe or "workflow"


def _get_file_extension(field_name: str) -> str:
    """Get appropriate file extension for code field.

    Args:
        field_name: Name of the code field

    Returns:
        File extension (e.g., ".py", ".js")
    """
    if field_name == "pythonCode":
        return ".py"
    elif field_name in ("jsCode", "code", "functionCode"):
        return ".js"
    else:
        return ".txt"


def _externalize_workflow_code(
    workflow: dict[str, Any],
    workflow_name: str,
    scripts_dir: Path,
) -> tuple[dict[str, Any], int]:
    """Externalize inline code from workflow nodes.

    Args:
        workflow: Workflow JSON object
        workflow_name: Name of the workflow
        scripts_dir: Directory to save script files

    Returns:
        Tuple of (modified_workflow, count_of_externalized_code_blocks)
    """
    import copy
    modified = copy.deepcopy(workflow)
    externalized_count = 0

    # Create workflow-specific scripts directory
    safe_workflow_name = _sanitize_filename(workflow_name)
    workflow_scripts_dir = scripts_dir / safe_workflow_name
    workflow_scripts_dir.mkdir(parents=True, exist_ok=True)

    nodes = modified.get("nodes", [])
    if not isinstance(nodes, list):
        return modified, 0

    for node in nodes:
        if not isinstance(node, dict):
            continue

        node_name = node.get("name", "unnamed")
        node_id = node.get("id", "no-id")
        parameters = node.get("parameters", {})

        if not isinstance(parameters, dict):
            continue

        # Check each code field
        for field_name in CODE_FIELD_NAMES:
            if field_name not in parameters:
                continue

            code_value = parameters[field_name]
            if not isinstance(code_value, str) or not code_value.strip():
                continue

            # Check if it's already an include directive
            if code_value.strip().startswith("@@n8n-gitops:include"):
                continue

            # Externalize this code
            safe_node_name = _sanitize_filename(node_name)
            extension = _get_file_extension(field_name)

            # Create filename: node-name_field-type.ext
            # Overwrite if it already exists (no counter)
            base_filename = f"{safe_node_name}_{field_name}{extension}"
            script_path = workflow_scripts_dir / base_filename

            # Write code to file (overwrite if exists)
            script_path.write_text(code_value)

            # Create include directive
            # Path relative to n8n/ directory
            relative_path = f"scripts/{safe_workflow_name}/{base_filename}"
            include_directive = f"@@n8n-gitops:include {relative_path}"

            # Replace inline code with directive
            parameters[field_name] = include_directive
            externalized_count += 1

            print(f"      â†’ Externalized {field_name} from node '{node_name}' to {relative_path}")

    return modified, externalized_count
